{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Recommendation System Documentation**\n",
        "\n",
        "### 1. Project Initialization\n",
        "\n",
        "> The pipeline begins by establishing a connection to the **OpenAI API** and loading the **AG News Dataset**. This dataset contains a collection of news articles categorized by their titles and descriptions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mr8IGUC5MXUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3npKPrCbGwMn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get(\"API_KEY\")\n",
        "BASE_URL = userdata.get(\"BASE_URL\")\n",
        "client = OpenAI(\n",
        "    api_key=\"API_KEY\",\n",
        "    base_url=\"BASE_URL\"\n",
        ")\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data (full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\n",
        "dataset_path = \"/content/AG_news_samples.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "n_examples = 5\n",
        "df.head(n_examples)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HlA_lXc7fajP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWdwI91qGwMr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# print the title, description, and label of each example\n",
        "for idx, row in df.head(n_examples).iterrows():\n",
        "    print(\"\")\n",
        "    print(f\"Title: {row['title']}\")\n",
        "    print(f\"Description: {row['description']}\")\n",
        "    print(f\"Label: {row['label']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Semantic Embedding & Local Caching\n",
        "\n",
        "> To transform text into a format a machine can understand, the system uses the `text-embedding-3-small` model. This converts each article description into a high-dimensional vector. To ensure efficiency, a local `.pkl` file acts as a database to store and reuse these embeddings."
      ],
      "metadata": {
        "id": "-ya8UIqIMyPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tJbAumwbGwMs"
      },
      "outputs": [],
      "source": [
        "# establish a cache of embeddings to avoid recomputing\n",
        "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
        "\n",
        "# set path to embedding cache\n",
        "embedding_cache_path = \"/content/recommendations_embeddings_cache.pkl\"\n",
        "\n",
        "# load the cache if it exists, and save a copy to disk\n",
        "try:\n",
        "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
        "except FileNotFoundError:\n",
        "    embedding_cache = {}\n",
        "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
        "    pickle.dump(embedding_cache, embedding_cache_file)\n",
        "\n",
        "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
        "def embedding_from_string(\n",
        "    string: str,\n",
        "    model: str = EMBEDDING_MODEL,\n",
        "    embedding_cache=embedding_cache\n",
        ") -> list:\n",
        "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
        "    if (string, model) not in embedding_cache.keys():\n",
        "        embedding_cache[(string, model)] = (string,model)\n",
        "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
        "            pickle.dump(embedding_cache, embedding_cache_file)\n",
        "    return embedding_cache[(string, model)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KLvWrlpGwMs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# as an example, take the first description from the dataset\n",
        "example_string = df[\"description\"].values[0]\n",
        "print(f\"\\nExample string: {example_string}\")\n",
        "\n",
        "# print the first 10 dimensions of the embedding\n",
        "example_embedding = embedding_from_string(example_string)\n",
        "print(f\"\\nExample embedding: {example_embedding[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Similarity Calculation (Cosine Distance)\n",
        "\n",
        "\n",
        "> Once articles are converted into vectors, the system calculates how \"related\" they are using **Cosine Similarity**.\n",
        "\n",
        "It measures the cosine of the angle between two vectors; a smaller angle indicates that the articles share a high degree of semantic similarity."
      ],
      "metadata": {
        "id": "N8k7DCf4NA5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_distance(a, b):\n",
        "    return 1 - np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def distances_from_embeddings(query_embedding, embeddings):\n",
        "    return [cosine_distance(query_embedding, emb) for emb in embeddings]\n",
        "\n",
        "def indices_of_nearest_neighbors_from_distances(distances):\n",
        "    return np.argsort(distances)\n"
      ],
      "metadata": {
        "id": "TseG7RGCCQgS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IwOxt32PGwMt"
      },
      "outputs": [],
      "source": [
        "def embedding_from_string(string: str, model=EMBEDDING_MODEL):\n",
        "    key = (string, model)\n",
        "\n",
        "    if key not in embedding_cache:\n",
        "        response = client.embeddings.create(\n",
        "            model=model,\n",
        "            input=string\n",
        "        )\n",
        "        embedding_cache[key] = response.data[0].embedding\n",
        "\n",
        "        with open(embedding_cache_path, \"wb\") as f:\n",
        "            pickle.dump(embedding_cache, f)\n",
        "\n",
        "    return embedding_cache[key]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_recommendations_from_strings(strings, index_of_source_string, k_nearest_neighbors=5):\n",
        "    embeddings = [embedding_from_string(s) for s in strings]\n",
        "\n",
        "    query_embedding = embeddings[index_of_source_string]\n",
        "\n",
        "    distances = distances_from_embeddings(query_embedding, embeddings)\n",
        "    sorted_indices = indices_of_nearest_neighbors_from_distances(distances)\n",
        "\n",
        "    print(\"Source article:\\n\", strings[index_of_source_string], \"\\n\")\n",
        "\n",
        "    count = 0\n",
        "    for i in sorted_indices:\n",
        "        if i == index_of_source_string:\n",
        "            continue\n",
        "        if count >= k_nearest_neighbors:\n",
        "            break\n",
        "\n",
        "        count += 1\n",
        "        print(f\"--- Recommendation {count} ---\")\n",
        "        print(strings[i])\n",
        "        print(f\"Distance: {distances[i]:.4f}\\n\")\n",
        "\n",
        "    return sorted_indices\n"
      ],
      "metadata": {
        "id": "DxzRitPYGo6T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nJ-azuDBGwMt"
      },
      "outputs": [],
      "source": [
        "article_descriptions = (\n",
        "    df[\"description\"]\n",
        "    .dropna()\n",
        "    .astype(str)\n",
        "    .tolist()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIDXg8CJGwMu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "chipset_security_articles = print_recommendations_from_strings(\n",
        "    strings=article_descriptions,  # let's base similarity off of the article description\n",
        "    index_of_source_string=1,  # let's look at articles similar to the second one about a more secure chipset\n",
        "    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Dimensionality Reduction via t-SNE\n",
        "\n",
        "\n",
        "> Raw embeddings are too complex for human visualization. To visualize how the articles cluster together, the system employs **t-SNE** (t-distributed Stochastic Neighbor Embedding) to \"squash\" high-dimensional data into a **2D plane**."
      ],
      "metadata": {
        "id": "5l-vtj75NJ4Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "t0P3KhqkGwMv"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def tsne_components_from_embeddings(embeddings, n_components=2, perplexity=30, random_state=42):\n",
        "    X = np.array(embeddings)\n",
        "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=random_state)\n",
        "    return tsne.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t9SwVDvjGwMw"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "def chart_from_components(components, labels, strings, width=600, height=500, title=\"\", category_orders=None):\n",
        "    df_plot = pd.DataFrame({\n",
        "        \"x\": components[:, 0],\n",
        "        \"y\": components[:, 1],\n",
        "        \"label\": labels,\n",
        "        \"text\": strings\n",
        "    })\n",
        "\n",
        "    fig = px.scatter(\n",
        "        df_plot,\n",
        "        x=\"x\",\n",
        "        y=\"y\",\n",
        "        color=\"label\",\n",
        "        hover_data=[\"text\"],\n",
        "        title=title,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        category_orders=category_orders\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oEDTu1EgGwMw"
      },
      "outputs": [],
      "source": [
        "def nearest_neighbor_labels(list_of_indices, k_nearest_neighbors=5):\n",
        "    labels = [\"Other\"] * len(article_descriptions)\n",
        "\n",
        "    source_index = list_of_indices[0]\n",
        "    labels[source_index] = \"Source\"\n",
        "\n",
        "    for i in range(1, k_nearest_neighbors + 1):\n",
        "        idx = list_of_indices[i]\n",
        "        labels[idx] = f\"Nearest neighbor (top {k_nearest_neighbors})\"\n",
        "\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrMrUEZiGwMw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "embeddings = [embedding_from_string(s) for s in article_descriptions]\n",
        "\n",
        "tsne_components = tsne_components_from_embeddings(embeddings)\n",
        "\n",
        "labels = df[\"label\"].tolist()\n",
        "\n",
        "chart_from_components(\n",
        "    components=tsne_components,\n",
        "    labels=labels,\n",
        "    strings=article_descriptions,\n",
        "    title=\"t-SNE components of article descriptions\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tony_blair_articles = print_recommendations_from_strings(\n",
        "    article_descriptions,\n",
        "    index_of_source_string=0,\n",
        "    k_nearest_neighbors=5\n",
        ")\n",
        "\n",
        "chipset_security_articles = print_recommendations_from_strings(\n",
        "    article_descriptions,\n",
        "    index_of_source_string=1,\n",
        "    k_nearest_neighbors=5\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KFncN_kLIX40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Interactive Visualization\n",
        "\n",
        "\n",
        "> The final stage uses **Plotly** to generate interactive scatter plots. Points are color-coded by category, allowing you to visually confirm that similar articles are grouped together in the vector space.\n"
      ],
      "metadata": {
        "id": "6HNeENrCNVNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tony_blair_labels = nearest_neighbor_labels(tony_blair_articles, 5)\n",
        "chipset_security_labels = nearest_neighbor_labels(chipset_security_articles, 5)\n",
        "\n",
        "chart_from_components(\n",
        "    components=tsne_components,\n",
        "    labels=tony_blair_labels,\n",
        "    strings=article_descriptions,\n",
        "    title=\"Nearest neighbors of the Tony Blair article\",\n",
        "    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]}\n",
        ")\n",
        "\n",
        "chart_from_components(\n",
        "    components=tsne_components,\n",
        "    labels=chipset_security_labels,\n",
        "    strings=article_descriptions,\n",
        "    title=\"Nearest neighbors of the chipset security article\",\n",
        "    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]}\n",
        ")\n"
      ],
      "metadata": {
        "id": "wLCoYiOiHswJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In conclusion, this project demonstrates a robust and scalable approach to building a **Content-Based Recommendation System** using modern LLM tools. By moving beyond simple keyword matching and into the realm of **Vector Embeddings**, the system can identify deep semantic relationships between news articles.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "* **Efficiency via Caching:** The implementation of a local pickle-based cache ensures that the system is both cost-effective and high-performing, avoiding redundant API calls for previously processed text.\n",
        "* **Mathematical Precision:** By utilizing **Cosine Similarity**, the engine effectively ranks articles based on their conceptual \"nearness\" in a high-dimensional vector space.\n",
        "* **Visual Validation:** The use of **t-SNE** allows us to translate complex, invisible mathematical relationships into a 2D map. This visual feedback confirms that the model is successfully grouping similar topics (like \"Tech\" or \"Politics\") together.\n",
        "* **Explainability:** The integration of interactive charts provides a \"glass-box\" view of the AI, making it easy to audit why specific recommendations were made for a given source article.\n",
        "\n",
        "---\n",
        "\n",
        "## Potential Next Steps\n",
        "\n",
        "To further enhance this system, one could:\n",
        "\n",
        "1. **Hybrid Filtering:** Combine these semantic embeddings with user behavior data (Collaborative Filtering) for more personalized results.\n",
        "2. **Vector Database:** Transition from a local `.pkl` file to a dedicated vector database (like Pinecone or Milvus) to handle millions of articles in real-time.\n",
        "3. **Cross-Lingual Support:** Leverage OpenAI's multi-lingual embedding capabilities to recommend related articles across different languages.\n"
      ],
      "metadata": {
        "id": "3Teh99qYNfqw"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}